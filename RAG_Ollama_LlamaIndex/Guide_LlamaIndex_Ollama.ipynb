{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test case 1: Talk with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rome grew into a powerful city-state in the Italian Peninsula, with a rich history and culture.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "resp = llm.complete(\"What did Rome grow? Be concise.\")\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test case 2: Simple conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20121/4177303745.py:7: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  service_context = ServiceContext.from_defaults(\n",
      "/home/rajatdulal/miniconda3/envs/langchain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import ServiceContext\n",
    "from llama_index.core.chat_engine import SimpleChatEngine\n",
    "\n",
    "llm = Ollama(model=\"llama3.1\")\n",
    "\n",
    "service_context = ServiceContext.from_defaults(\n",
    "    llm=llm,\n",
    "    embed_model=\"local:BAAI/bge-small-en-v1.5\", \n",
    ")\n",
    "\n",
    "chat_engine = SimpleChatEngine.from_defaults(service_context=service_context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chat_engine.chat(\"Hi, my name is Mirna\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chat_engine.chat(\"Can you tell what my name is?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Analysis:**\n",
      "\n",
      "The passage highlights the growing influence of China in the Indo-Pacific region, particularly in the South China Sea. The rise of China's assertiveness and its Belt and Road Initiative is impacting regional power structures and leading other nations to strengthen their own ties and seek counterbalancing influence.\n",
      "\n",
      "**Key Points:**\n",
      "\n",
      "* **China's Assertiveness:** China's assertive stance in the South China Sea and its Belt and Road Initiative are reshaping the region's power structures.\n",
      "* **India, Japan, and Australia's Response:** India, Japan, and Australia are responding by strengthening their own ties and seeking to counterbalance China's influence.\n",
      "* **Counterbalancing China's Influence:** Other nations are working together to establish a more balanced and peaceful order in the Indo-Pacific.\n",
      "\n",
      "**Additional Insights:**\n",
      "\n",
      "* The passage also suggests that the Indo-Pacific region is becoming a major battleground for global power competition.\n",
      "* The rise of China is transforming the geopolitical landscape of the region, making it more complex and unpredictable.\n",
      "* The passage emphasizes the need for international cooperation and collaboration to address the challenges and opportunities presented by China's growing influence.\n",
      "\n",
      "**Key Questions for Further Consideration:**\n",
      "\n",
      "* How can other nations effectively respond to China's assertiveness?\n",
      "* What are the potential consequences of a fully developed Indo-Pacific region under Chinese control?\n",
      "* How can international cooperation be strengthened to address the challenges posed by China's influence?\n"
     ]
    }
   ],
   "source": [
    "print(chat_engine.chat(\"\"\"The Indo-Pacific is becoming a key battleground for influence, especially with the rise of China. \n",
    "                       Their assertive stance in the South China Sea, coupled with their Belt and Road Initiative, is reshaping regional power structures. \n",
    "                       Nations like India, Japan, and Australia are responding by strengthening their own ties, seeking to counterbalance China’s influence.\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The passage mentions the following countries that are looking to counterbalance China's influence:\n",
      "\n",
      "* India\n",
      "* Japan\n",
      "* Australia\n"
     ]
    }
   ],
   "source": [
    "print(chat_engine.chat(\"Which countries are looking to counterbalance China?\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#test 3: Simple RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    set_global_service_context,\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma:2b\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20187/4212152171.py:14: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  .from_defaults(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reads pdfs at \"./\" path\n",
    "documents = (\n",
    "    SimpleDirectoryReader(\n",
    "        input_dir = './',\n",
    "        required_exts = [\".pdf\"])\n",
    "        .load_data()\n",
    ")\n",
    "\n",
    "# ServiceContext is a bundle of commonly used \n",
    "# resources used during the indexing and \n",
    "# querying stage \n",
    "service_context = (\n",
    "    ServiceContext\n",
    "    .from_defaults(\n",
    "        llm=llm, \n",
    "        embed_model=\"local:BAAI/bge-small-en-v1.5\", \n",
    "        chunk_size=300\n",
    "    )\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "# Node represents a “chunk” of a source Document\n",
    "nodes = (\n",
    "    service_context\n",
    "    .node_parser\n",
    "    .get_nodes_from_documents(documents)\n",
    ")\n",
    "\n",
    "# offers core abstractions around storage of Nodes, \n",
    "# indices, and vectors\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "# Create the vectorstore index\n",
    "index = (\n",
    "    VectorStoreIndex\n",
    "    .from_documents(\n",
    "        documents, \n",
    "        storage_context=storage_context, \n",
    "        llm=llm\n",
    "        )\n",
    ")\n",
    "query_engine = index.as_query_engine()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure. The countries that are trying to counterbalance China are India, Japan, and Australia.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the index\n",
    "query=\"\"\"Which countries are trying to counterbalance the China?\"\"\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sure, here's the answer to the query:\n",
      "\n",
      "Some countries are strengthening their own ties and seeking to counterbalance China's influence. This is evident in the Quad, which is a grouping of the United States, India, Japan, and Australia, who are working to maintain peace and stability in the Indo-Pacific region. The Quad aims to prevent a military confrontation between China and its allies. Additionally, countries like India and Japan are strengthening their partnerships with each other to counter China's influence.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the index\n",
    "query=\"\"\"How are some countries trying to counterbalance the China?\"\"\"\n",
    "response = query_engine.query(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms.ollama import Ollama\n",
    "# from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "# from llama_index.core import Settings\n",
    "\n",
    "# Settings.llm = Ollama(model=\"llama2\", request_timeout=120.0)\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=\"BAAI/bge-small-en-v1.5\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#task 4:Simple agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Running step 76d68232-eeab-417c-81a7-928f01942214. Step input: Add the numbers 3 and 2\n",
      "\u001b[1;3;38;5;200mThought: (Implicit) I can answer without any more tools!\n",
      "Answer: **Observation:** tool response\n",
      "```\n",
      "Observation: add_numbers_three_fn\n",
      "```\n",
      "\u001b[0m**Observation:** tool response\n",
      "```\n",
      "Observation: add_numbers_three_fn\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core.agent import ReActAgent\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "def add_numbers_three_fn(a : int, b: int) -> int:\n",
    "    \"\"\"Adds two numbers and a constant 3 and returns the result\"\"\"\n",
    "    return a+b+3\n",
    "\n",
    "tools = [\n",
    "    FunctionTool.from_defaults(fn=add_numbers_three_fn)\n",
    "]\n",
    "\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "agent = ReActAgent.from_tools(tools, llm=llm, verbose=True)\n",
    "response = agent.chat(\"Add the numbers 3 and 2\")\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example #5 — Conversational RAG\n",
    "Here, you will do the same from Example #3, using a different method when instantiating the chat_engine object, that isindex.as_chat_engine instead of index.as_query_engine.\n",
    "\n",
    "Now, you can ask follow-up questions, and the chat engine will be able to handle memory for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22447/3573117386.py:25: DeprecationWarning: Call to deprecated class method from_defaults. (ServiceContext is deprecated, please use `llama_index.settings.Settings` instead.) -- Deprecated since version 0.10.0.\n",
      "  .from_defaults(\n",
      "/home/rajatdulal/miniconda3/envs/langchain/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import (\n",
    "    ServiceContext,\n",
    "    SimpleDirectoryReader,\n",
    "    StorageContext,\n",
    "    VectorStoreIndex,\n",
    "    set_global_service_context,\n",
    ")\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"gemma:2b\")\n",
    "\n",
    "# Reads pdfs at \"./\" path\n",
    "documents = (\n",
    "    SimpleDirectoryReader(\n",
    "        input_dir = './',\n",
    "        required_exts = [\".pdf\"])\n",
    "        .load_data()\n",
    ")\n",
    "\n",
    "# ServiceContext is a bundle of commonly used \n",
    "# resources used during the indexing and \n",
    "# querying stage \n",
    "service_context = (\n",
    "    ServiceContext\n",
    "    .from_defaults(\n",
    "        llm=llm, \n",
    "        embed_model=\"local:BAAI/bge-small-en-v1.5\", \n",
    "        chunk_size=300\n",
    "    )\n",
    ")\n",
    "set_global_service_context(service_context)\n",
    "\n",
    "# Node represents a “chunk” of a source Document\n",
    "nodes = (\n",
    "    service_context\n",
    "    .node_parser\n",
    "    .get_nodes_from_documents(documents)\n",
    ")\n",
    "\n",
    "# offers core abstractions around storage of Nodes, \n",
    "# indices, and vectors\n",
    "storage_context = StorageContext.from_defaults()\n",
    "storage_context.docstore.add_documents(nodes)\n",
    "\n",
    "# Create the vectorstore index\n",
    "index = (\n",
    "    VectorStoreIndex\n",
    "    .from_documents(\n",
    "        documents, \n",
    "        storage_context=storage_context, \n",
    "        llm=llm\n",
    "        )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "chat_engine = index.as_chat_engine(chat_mode=\"context\", verbose = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat_engine.chat(\"Hi, my name is Rajat\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Rajat. It's nice to meet you too!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Assistant: Hello Mirna! It's great to meet you. What can I help with today?\n",
    "response = chat_engine.chat(\"What is my name?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm unable to access external information or engage in real-time conversations, so I cannot provide insights on the conversation between Ram and Sita.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Query the index\n",
    "query=\"\"\"please give insights on the conversation between Ram and Sita\"\"\"\n",
    "response = chat_engine.chat(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
